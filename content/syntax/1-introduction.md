---
layout: default
usemathjax: true
permalink: /syntax/ch1
---

# Introduction to Syntax

As a **cognitive science**, 
language is vital to how abstract notions are conceived and expressed.

The dominant model of syntax is **Generative Grammar** (by Chomsky et al.). There are 2 approaches:
-  Principles and Parameters
-  Minimalism (more recent)

## Generative Grammar

Thesis: The subconcious operates a set of procedures yielding sentences.
These *procedures* are called **rules**.

The method of studying syntax involves 3 steps in the cycle:

1. Gathering data
2. Generalizing information from data
3. Developing *hypotheses*

Hypotheses are **proposed rules**, which hold until *proven otherwise* by data.
Hence hypotheses must be **falsifiable**.

Whether rules actually exist or not is an ontological question; for our purposes
it suffices to say that rules merely form a model of our psychology of language.

### Example of scientific method

*Anaphor*: noun in the form of `(pronoun)-self`

1. Hyp 1: Anaphors must have an *antecedent* and agree in *gender*.
   1. Bill kissed himself.
   1. *Bill kissed herself.
   1. *Jane kissed himself.
   1. Jane kissed herself.
   1. *Kissed himself.
2. Hyp 2: Anaphors must agree in *gender* and *number*.
   1. The Joneses think themselves the best family on the block.
   1. *The Joneses think himself the wealthiest on the block.
   1. Gary and Kevin ran themselves into exhaustion.
   1. *Gary and Kevin ran himself into exhaustion.
3. Hyp 3: Anaphors must also agree in *person*.
   1. *The man believed myself the best at sports.
   1. *The man believed themselves the best at sports.
   1. The man believed himself the best at sports.

Corpora are insufficient as data sources for linguists. To make hypotheses useful, 
ungrammatical/non-well formed sentences must be used (falsifying purposes).

**Knowledge of native language** is **subconscious**. 
It allows us to perform the **grammaticality judgement task** based on our **intuition**.

> Q: Can intuition be considered scientific data? 
> - It is a real psychological effect
> - It is replicable under test conditions

## Competence vs Performance

- Garden path sentences: grammatically correct statements that are hard to parse.
- *Centre embedding*:
  - Cheese mice love stinks
  - #Cheese mice cats catch love stinks
  - Stacking too many **reduced** relative clauses (not relative marker) makes it hard to understand
  - Though grammatical, indicates **constraints** on either/both:
    - short term memory
    - mental ability to break apart sentences

**Competence**: What we **know** about the language.

**Performance**: The actual kinds of language being produced.

# Where do rules come from

## Learning vs Acquisition

**Learning**: Conscious knowledge

**Acquiring**: Subconscious knowledge

Classes in formal grammar of a foreign language fail abysmally to train people
but immersion in an environment allows for the subconscious to acquire.

**Innate**: Chomsky claims that many facts about Language itself is **instinctual** or innate.

- Universal Grammar (UG)
- Innateness "proof":
  - Syntax is a productive, recursive and infinite system.
  - (?) *Productive and infinite systems are unlearnable*.
  - It follows that some parts of syntax must be unlearnable and thus innate.
- Example of why infinite systems are unlearnable:
  - understanding of language is based on **mapping** of representation to situation.
  - *Since there are infinite stimuli/situations and infinite representations*
  - The representation-situation mapping cannot be exhaustively defined.
  - Hence humans must already "know" some of this mapping from birth
  - This is **the logical argument of Language Acquisition**
  - The underdetermination of data

## Explaining language variation

Grammars of languages differ because of innate **parameters** that select between variants.

example: Word order (V-S-O and variants)

**Adequacy** levels:
- Observationally adequate: data in corpus accounted for and nothing more
- Descriptively adequate: accounts for real world data and native speaker judgements
- Explanatorily adequate: accounts also how children can acquire this language

# Parts of Speech

aka Word Classes. Traditionally includes:

1. Lexical Categories: Open Class [can take on new coinages]
    1. [Nouns (N)](/notes-blog/sos/ch2#nouns)
    2. [Verbs (V)](/notes-blog/sos/ch2#verbs)
    3. [Adjectives (Adj)](/notes-blog/sos/ch2#adjectives)
    4. [Adverbs (Adv)](/notes-blog/sos/ch2#adverbs)
2. Functional Categories: Closed class [fixed set]
    1. [Determiners (D)](/notes-blog/sos/ch2#determiners)
    2. [Prepositions (P)](/notes-blog/sos/ch2#preposition)
    3. Conjunctions (CONJ)
    4. Tense (T)
    5. Negation (Neg)
    6. Complementizers (C)

The POS (part of speech) of a word is *determined by its place in the sentence and its morphology*. 
(Proof: you can identify which POS of a sentence with nonsense words replacing all **lexical** words).

## Distributional criteria

Morphological distribution:
- **derivational** morphemes: affixes making words of other words often resulting in a *different POS*
  - -tion
  - -al
  - -ally
  - -ed
- **inflectional** morphemes: they only attach to certain categories
  - -est has to attach to an adjective already, and doesn't change it to another POS
    - big; biggest; love; *lovest;

## Subcategories
